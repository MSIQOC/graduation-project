{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vanilla GAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMf91gDN6d0MvSuq/j4XaRm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6j-ROGwa8A38"},"source":["# reference to : https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","\n","from keras.layers import Input\n","from keras.models import Model, Sequential\n","from keras.layers.core import Dense, Dropout\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.datasets import mnist\n","from keras.optimizers import Adam\n","from keras import initializers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4KcC-7j8e8q"},"source":["tf.random.set_seed(42)\n","np.random.seed(42)\n","# 사용할 라이브러리 선언\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","import tensorflow as tf\n","import seaborn as sns\n","from pylab import rcParams\n","from sklearn.model_selection import train_test_split\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras import regularizers\n","\n","#for data preprocessing\n","from sklearn.decomposition import PCA\n","\n","#for modeling\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.ensemble import IsolationForest\n","from sklearn.model_selection import train_test_split #training and testing data split\n","from keras.layers import Input, Dense\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras import regularizers\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from sklearn.preprocessing import StandardScaler\n","#filter warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","RANDOM_SEED = 42\n","random_dim = 121"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLIuVjvK8jQ9"},"source":["data=pd.read_csv(\"/content/drive/My Drive/merge_real_total.csv\", header=None)  #정상 데이터 불러오기\n","data2 = pd.read_csv(\"/content/drive/My Drive/merge_paf_total.csv\",header=None) #비정상 데이터 불러오기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrzKO0OR8stp"},"source":["RANDOM_SEED=42\n","X_train,X_test=train_test_split(data,test_size=0.2,random_state=RANDOM_SEED) #train_test_split을 이용하여 전체 데이터에서 train용과 test용을 분리 (real data)\n","X_train = X_train.astype(float) / 255\n","X_train=X_train.dropna(axis=1) \n","X_test = X_test.astype(float) / 255\n","X_test=X_test.dropna(axis=1)\n","input_dim = X_train.shape[1] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-9u925J82KQ"},"source":["RANDOM_SEED = 42\n","X_train2,X_test2=train_test_split(data2,test_size=0.2,random_state=RANDOM_SEED) #train_test_split을 이용하여 전체 데이터에서 train용과 test용을 분리 (paf data)\n","X_train2 = X_train2.astype(float) / 255\n","X_train2=X_train2.dropna(axis=1) \n","tmp=X_train2.to_numpy()\n","X_test2 = X_test2.astype(float) / 255\n","X_test2=X_test2.dropna(axis=1) #"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jl4tjd7483ay"},"source":["def get_generator():\n","        model = Sequential()\n","        model.add(layers.Dense(256, input_shape=(random_dim,)))\n","        model.add(layers.LeakyReLU(alpha=0.2))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.Dense(512))\n","        model.add(layers.LeakyReLU(alpha=0.2))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.Dense(1024))\n","        model.add(layers.LeakyReLU(alpha=0.2))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.Dense(121, activation='tanh'))\n","        model.compile(loss='binary_crossentropy', optimizer='Adam')\n","        return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8LQYJDw9Gcm"},"source":["def get_discriminator():\n","      model = Sequential()\n","      model.add(layers.Flatten(input_shape=(random_dim,)))\n","      model.add(layers.LeakyReLU(0.2))\n","      model.add(layers.Dense(512))\n","      model.add(layers.LeakyReLU(alpha=0.2))\n","      model.add(layers.Dense(256))\n","      model.add(layers.LeakyReLU(alpha=0.2))\n","      model.add(layers.Dense(1, activation='sigmoid'))\n","      model.compile(loss='binary_crossentropy', optimizer='Adam')\n","      return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWAiLkVV9HVR"},"source":["def get_gan_network(discriminator, random_dim, generator):\n","    discriminator.trainable = False\n","    gan_input = Input(shape=(random_dim,))\n","    x = generator(gan_input)\n","    gan_output = discriminator(x)\n","    gan = Model(inputs=gan_input, outputs=gan_output)\n","    gan.compile(loss='binary_crossentropy', optimizer='adam')\n","    return gan"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xebU3T879IPi"},"source":["epochs=100\n","batch_size=128\n","# train 데이터를 128 사이즈의 batch 로 나눕니다.\n","batch_count = X_train.shape[0] // batch_size\n","\n","# 우리의 GAN 네트워크를 만듭니다.\n","generator = get_generator()\n","discriminator = get_discriminator()\n","gan = get_gan_network(discriminator, random_dim, generator)\n","k = 0\n","for e in range(1, epochs+1):\n","    for _ in tqdm(range(batch_count)):\n","        noise = np.random.normal(0, 1, size=[batch_size, random_dim]) #이걸 사용하는 이유는 0~1사이의 값을 가지는 난수를 발생시키는(생성자에 구분시키는 능력주려고)\n","        image_batch = X_train.to_numpy()[np.random.randint(0, X_train.to_numpy().shape[0], size=batch_size)]\n","        \n","        #print(image_batch)\n","        #l, image_batch = get_batch(k, batch_size)\n","        #print('changed:')\n","        #print(image_batch)\n","        #k = l\n","        \n","        generated_images = generator.predict(noise)\n","        X = np.concatenate([image_batch, generated_images]) #concatenate --> np 합쳐준다\n","        y_dis = np.zeros(2*batch_size)\n","        y_dis[:batch_size] = 0.9\n","        discriminator.trainable = True\n","        discriminator.fit(X, y_dis, validation_split=0.3, verbose=0)\n","\n","        # Generator 학습\n","        noise = np.random.normal(0, 1, size=[batch_size, random_dim]) #generator는 가짜를 생성하도록 학습\n","        y_gen = np.ones(batch_size)\n","        gan.fit(noise, y_gen,shuffle=True,validation_split=0.3,verbose=0) #validation을 위하여 fit사용 validation은 입력데이터에서 6:4의 비율\n","model_json = generator.to_json() \n","with open(\"/content/drive/My Drive/vanillagan_ecg.json\", \"w\") as json_file: \n","  json_file.write(model_json)\n","generator.save_weights(\"/content/drive/My Drive/vanillagan_ecg.h5\") \n","print(\"Saved model to disk\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3OHVQWC9yBq"},"source":["from tensorflow.compat.v2.keras.models import model_from_json\n","\n","\n","json_file = open(\"/content/drive/My Drive/vanillagan_ecg.json\", \"r\") \n","loaded_model_json = json_file.read() \n","json_file.close()\n","\n","loaded_model = model_from_json(loaded_model_json)\n","\n","loaded_model.load_weights(\"/content/drive/My Drive/vanillagan_ecg.h5\") \n","print(\"Loaded model from disk\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIOwfgq_-z-G"},"source":["generated_images2 = loaded_model.predict(X_test)  #real로 훈련된 생성자에 진짜 데이터를 입력으로 준다.\n","generated_images22 = loaded_model.predict(X_test2)  #real로 훈련된 생성자에 가짜 데이터를 입력으로 준다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHiheP63BVKn"},"source":["import sklearn\n","x_sk=sklearn.metrics.pairwise.cosine_similarity(X_test.to_numpy(), generated_images2, dense_output=True) #real 재생성과 real test data 사이의 유사도 비교\n","# print(x_sk)\n","print(x_sk.mean())\n","x_sk=sklearn.metrics.pairwise.cosine_similarity(X_test.to_numpy(), generated_images22, dense_output=True) #paf 재생성과 paf test data 사이의 유사도 비교\n","# print(x_sk)\n","print(x_sk.mean())"],"execution_count":null,"outputs":[]}]}